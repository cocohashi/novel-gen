#### PRE-PROCESSING
## Paths
EPUB_DIR_PATH = "books/epub"
TXT_DIR_PATH = "books/txt"
CHUNK_DIR_PATH = "books/chunks"

## Parameters
CHUNK_LENGTH = 3000

#### TEXT-GENERATION
## Paths
MODEL_DIR_NAME = "models"

## Models
MODEL_BLOOM = "bloom-560m"

## Text Generation Config

# Documentation: https://huggingface.co/docs/transformers/main_classes/text_generation

# (int, optional) — The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
MAX_NEW_TOKENS = 100

#  (float, optional, defaults to model.config.temperature or 1.0 if the config does not set any value)
# — The value used to module the next token probabilities.
TEMPERATURE = 1.2

# (int, optional, defaults to model.config.top_k or 50 if the config does not set any value)
# — The number of highest probability vocabulary tokens to keep for top-k-filtering.
# TOP_K = 50

# (float, optional, defaults to model.config.top_p or 1.0 if the config does not set any value) —
# If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher
# are kept for generation.
TOP_P = 0.8

# (float, optional) — The maximum amount of time you allow the computation to run for in seconds.
# Generation will still finish the current pass after allocated time has been passed.
MAX_TIME = 60

# (bool, optional, defaults to True)
# — Whether or not the model should use the past last key/values attentions (if applicable to the model)
# to speed up decoding.
USE_CACHE = True